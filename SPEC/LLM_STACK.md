# Local LLM Stack (Ollama + OpenAI‑proxy)
- Compose: ollama + litellm on 127.0.0.1:11434/4000
- Default model: qwen2.5:7b‑instruct‑q4_K_M
- Acceptance: T1..T3 (offline models/latency), T4..T9 (profiles/switch/fallback), T10 (LoRA A/B)
